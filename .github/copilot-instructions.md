# Copilot Instructions for Data-Engineering

## Project Overview
This is a **data pipeline project** focused on ingesting NYC taxi trip data into PostgreSQL. The core pipeline extracts compressed CSV data from GitHub releases, transforms it with pandas, and loads it into a Postgres database using SQLAlchemy.

## Architecture & Key Components

### Data Flow
```
NYC TLC CSV (GitHub) → pandas (read_csv chunked) → PostgreSQL (SQLAlchemy)
```

### Main Modules
- **[ingest_data.py](pipeline/ingest_data.py)**: Core ETL script using Click CLI with options for database connection, year/month parameters, and chunked processing
- **[pipeline.py](pipeline/pipeline.py)**: Simple prototype script demonstrating data transformation and Parquet export
- **[main.py](pipeline/main.py)**: Placeholder entry point
- **Jupyter Notebooks**: Used for exploratory analysis (see `pipeline/notebook.ipynb`)

### External Dependencies
- **PostgreSQL 18**: Backend database (via docker-compose, default: `ny_taxi` DB, user `root`, pass `root`)
- **pgAdmin**: Database GUI on port 8085 for debugging
- **Python 3.13+**: Uses `uv` for dependency management
- **NYC TLC Data**: Yellow taxi data downloaded directly from DataTalksClub GitHub releases

## Development Workflow

### Setup
1. **Dependency Management**: Uses `uv` (not pip). Update [pyproject.toml](pipeline/pyproject.toml) for dependencies
2. **Virtual Environment**: Managed by `uv sync` during Docker build
3. **Docker Environment**: 
   - Run `docker-compose up` to start Postgres and pgAdmin services
   - pgAdmin accessible at `localhost:8085` (admin@admin.com / root)

### Running the Pipeline
```bash
# Via Docker container
docker build -t nyc-taxi-ingest -f pipeline/Dockerfile .
docker run --network pipeline_default nyc-taxi-ingest \
  --pg-user root --pg-pass root --pg-host pgdatabase \
  --pg-port 5432 --pg-db ny_taxi --target-table yellow_tripdata \
  --year 2024 --month 1

# Local development (requires uv installed)
cd pipeline
uv run python ingest_data.py --pg-user root --pg-pass root \
  --pg-host localhost --pg-port 5432 --pg-db ny_taxi \
  --target-table yellow_tripdata --year 2024 --month 1
```

### Key CLI Parameters (ingest_data.py)
- `--pg-user`, `--pg-pass`, `--pg-host`, `--pg-port`, `--pg-db`: Database connection (required)
- `--target-table`: Table name (required)
- `--year`, `--month`: Data version (required, formatted as YYYY-MM in URL)
- `--chunksize`: Batch size for reads (default: 100,000)

## Code Patterns & Conventions

### Data Schema
NYC taxi data uses a fixed dtype mapping in [ingest_data.py](pipeline/ingest_data.py#L5-L20):
- Location IDs, vendor ID, payment type → `Int64` (nullable integers)
- Monetary amounts → `float64`
- Datetime fields → `tpep_pickup_datetime`, `tpep_dropoff_datetime` (parsed as dates)
- String fields → `string` type

When processing taxi data, always apply this dtype dict to avoid type inference issues.

### ETL Pattern
Use the chunked iterator pattern (see [ingest_data.py](pipeline/ingest_data.py#L56-L76)):
1. **First chunk**: Create table with `if_exists='replace'` using `head(0)` schema
2. **Subsequent chunks**: Append with `if_exists='append'`
3. Use `tqdm` for progress tracking on large datasets
4. Print feedback on insertion size for monitoring

### Database Connection
- Use SQLAlchemy engine: `create_engine(f'postgresql://{user}:{pass}@{host}:{port}/{db}')`
- Connection string format is critical—validate host/port before pipeline runs

## Testing & Debugging
- **pgAdmin**: Query database at `localhost:8085` to verify data loads
- **Notebooks**: Use Jupyter for exploratory analysis; stored in `pipeline/`
- **Logging**: Pipeline prints chunk insertion confirmations; redirect stdout to log files if needed
- **Data files**: `taxi_zone_lookup.csv` is available for reference data joins

## File Structure Notes
- **Homeworks/wk1/**: Homework assignment directory (separate from pipeline)
- **test/**: Test files (currently placeholder scripts)
- **pipeline.egg-info/**: Generated by setuptools; auto-created, don't edit manually
- **my docker run**: Notes file (likely command examples)

## Integration Points
- Data source: `https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_YYYY-MM.csv.gz`
- Docker networking: Uses `pipeline_default` network when running via docker-compose
- Database: Postgres on `pgdatabase` (service name) or `localhost:5432` (local development)

## When Modifying the Pipeline
1. **Adding columns**: Update the `dtype` dict in `ingest_data.py` for schema consistency
2. **Changing data source**: Update the `prefix` URL variable
3. **New features**: Use Click `@click.option` decorators (avoid argparse)
4. **Dependencies**: Run `uv add <package>` to update `pyproject.toml`
5. **Docker changes**: Rebuild image after pyproject.toml changes: `docker build --no-cache ...`
